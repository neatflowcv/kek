# ADR-001: 번역 모델 크기 제한 (5B 이하)

## 상태

승인됨

## 컨텍스트

이 프로젝트는 저자원 환경에서도 실행 가능한 번역 시스템을 목표로 한다.

- 고성능 GPU가 없는 환경 (소비자용 GPU, CPU 전용 서버)
- 엣지 디바이스 배포 가능성
- 비용 효율적인 클라우드 인스턴스 활용

## 결정

번역 모델은 **5B(50억) 파라미터 이하**로 제한한다.

### 선정 기준

1. **크기**: 5B 이하
2. **특화**: 범용 LLM이 아닌 번역 특화 모델
3. **한국어 지원**: 한국어-영어 양방향 번역 가능

### 현재 선택

`facebook/nllb-200-distilled-600M` (600M 파라미터)

### 대안 후보

| 모델 | 크기 | 출시 | 비고 |
|------|------|------|------|
| `tencent/HY-MT1.5-1.8B` | 1.8B | 2025.12 | WMT25 우승, 최신 |
| `ModelSpace/GemmaX2-28-2B-v0.1` | 3B | 2025.02 | Gemma2 기반 |
| `nllb-200-distilled-1.3B` | 1.3B | 2022 | 현재 모델 업그레이드 |
| `nllb-200-3.3B` | 3.3B | 2022 | 품질 우선 시 |

## 결과

### 장점

- 8GB VRAM GPU에서 실행 가능
- 빠른 추론 속도
- 낮은 운영 비용

### 단점

- 대형 모델 대비 번역 품질 제한
- 복잡한 문맥/도메인 특화 번역에서 한계

## 참고

- [대안 모델 목록](../alternative-models.md)
